{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.12 64-bit ('learning': conda)",
      "metadata": {
        "interpreter": {
          "hash": "566c0a97317f6f88d4bc5f478002f1c75c862f0281a52c0ded6c5ead36971532"
        }
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install dgl"
      ],
      "metadata": {
        "id": "D90dV6XyM4wi",
        "outputId": "f500e906-1b95-4ab5-88d3-0b3718bcaf9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dgl\n",
            "  Downloading dgl-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 4.9 MB/s \n",
            "\u001b[?25hCollecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[K     |████████████████████████████████| 281 kB 29.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.7.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl) (4.64.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Installing collected packages: psutil, dgl\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed dgl-0.9.0 psutil-5.9.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6C7b4AikMqVL",
        "outputId": "256bd9db-4ea4-4c87-a874-7794feddaa18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import dgl\n",
        "\n",
        "np.random.seed(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XwFQNdtPMqVO"
      },
      "outputs": [],
      "source": [
        "def leaky_relu(z):\n",
        "    return np.where(z > 0, z, z * 0.01)\n",
        "\n",
        "def softmax(z):\n",
        "    if len(z.shape) > 1:\n",
        "        # Softmax for matrix\n",
        "        max_matrix = np.max(z, axis=0)\n",
        "        stable_z = z - max_matrix\n",
        "        e = np.exp(stable_z)\n",
        "        a = e / np.sum(e, axis=0, keepdims=True)\n",
        "    else:\n",
        "        # Softmax for vector\n",
        "        vector_max_value = np.max(z)\n",
        "        a = (np.exp(z - vector_max_value)) / sum(np.exp(z - vector_max_value))\n",
        "\n",
        "    assert a.shape == z.shape\n",
        "\n",
        "    return a\n"
      ]
    },
    {
      "source": [
        "### Graph and Weight Matrix Generation"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhtDL1aeMqVP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pASfNr6qMqVP",
        "outputId": "ec9f444a-fa2e-4b84-ab57-ca68d045aebb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "----- One-hot vector representation of nodes. Shape(n,n)\n",
            "\n",
            "[[0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]]\n",
            "\n",
            "\n",
            "----- Embedding dimension\n",
            "\n",
            "3\n",
            "\n",
            "\n",
            "----- Weight Matrix. Shape(emb, n)\n",
            "\n",
            "[[-0.4294049   0.57624235 -0.3047382  -0.11941829 -0.12942953]\n",
            " [ 0.19600584  0.5029172   0.3998854  -0.21561317  0.02834577]\n",
            " [-0.06529497 -0.31225734  0.03973776  0.47800217 -0.04941563]]\n",
            "\n",
            "\n",
            "----- Adjacency Matrix (undirected graph). Shape(n,n)\n",
            "\n",
            "[[1 1 1 0 1]\n",
            " [1 1 1 1 1]\n",
            " [1 1 1 1 0]\n",
            " [0 1 1 1 1]\n",
            " [1 1 0 1 1]]\n"
          ]
        }
      ],
      "source": [
        "print('\\n\\n----- One-hot vector representation of nodes. Shape(n,n)\\n')\n",
        "X = np.eye(5, 5)\n",
        "n = X.shape[0]\n",
        "np.random.shuffle(X)\n",
        "print(X)\n",
        "\n",
        "print('\\n\\n----- Embedding dimension\\n')\n",
        "emb = 3\n",
        "print(emb)\n",
        "\n",
        "print('\\n\\n----- Weight Matrix. Shape(emb, n)\\n')\n",
        "W = np.random.uniform(-np.sqrt(1. / emb), np.sqrt(1. / emb), (emb, n))\n",
        "print(W)\n",
        "\n",
        "print('\\n\\n----- Adjacency Matrix (undirected graph). Shape(n,n)\\n')\n",
        "A = np.random.randint(2, size=(n, n))\n",
        "np.fill_diagonal(A, 1)  \n",
        "A = (A + A.T)\n",
        "A[A > 1] = 1\n",
        "print(A)"
      ]
    },
    {
      "source": [
        "### Linear Transformation"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "nZHe1lpwMqVP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jR_f6KRnMqVQ",
        "outputId": "cc8e5f14-9d7c-4cce-9427-c691c9ba8ebc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "----- Linear Transformation. Shape(n, emb)\n",
            "\n",
            "[[-0.3047382   0.3998854   0.03973776]\n",
            " [ 0.57624235  0.5029172  -0.31225734]\n",
            " [-0.12942953  0.02834577 -0.04941563]\n",
            " [-0.4294049   0.19600584 -0.06529497]\n",
            " [-0.11941829 -0.21561317  0.47800217]]\n"
          ]
        }
      ],
      "source": [
        "# equation (1)\n",
        "print('\\n\\n----- Linear Transformation. Shape(n, emb)\\n')\n",
        "z1 = X.dot(W.T)\n",
        "print(z1)"
      ]
    },
    {
      "source": [
        "### Transformer: Additive Attention Mechanism"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "RXq7cRPQMqVQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kJwQzW0YMqVR",
        "outputId": "59dc756b-2253-4079-dca9-bf1da9b82c40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "----- Concat hidden features to represent edges. Shape(len(emb.concat(emb)), number of edges)\n",
            "\n",
            "[[-0.3047382   0.3998854   0.03973776 -0.3047382   0.3998854   0.03973776]\n",
            " [-0.3047382   0.3998854   0.03973776  0.57624235  0.5029172  -0.31225734]\n",
            " [-0.3047382   0.3998854   0.03973776 -0.12942953  0.02834577 -0.04941563]\n",
            " [-0.3047382   0.3998854   0.03973776 -0.11941829 -0.21561317  0.47800217]\n",
            " [ 0.57624235  0.5029172  -0.31225734 -0.3047382   0.3998854   0.03973776]\n",
            " [ 0.57624235  0.5029172  -0.31225734  0.57624235  0.5029172  -0.31225734]\n",
            " [ 0.57624235  0.5029172  -0.31225734 -0.12942953  0.02834577 -0.04941563]\n",
            " [ 0.57624235  0.5029172  -0.31225734 -0.4294049   0.19600584 -0.06529497]\n",
            " [ 0.57624235  0.5029172  -0.31225734 -0.11941829 -0.21561317  0.47800217]\n",
            " [-0.12942953  0.02834577 -0.04941563 -0.3047382   0.3998854   0.03973776]\n",
            " [-0.12942953  0.02834577 -0.04941563  0.57624235  0.5029172  -0.31225734]\n",
            " [-0.12942953  0.02834577 -0.04941563 -0.12942953  0.02834577 -0.04941563]\n",
            " [-0.12942953  0.02834577 -0.04941563 -0.4294049   0.19600584 -0.06529497]\n",
            " [-0.4294049   0.19600584 -0.06529497  0.57624235  0.5029172  -0.31225734]\n",
            " [-0.4294049   0.19600584 -0.06529497 -0.12942953  0.02834577 -0.04941563]\n",
            " [-0.4294049   0.19600584 -0.06529497 -0.4294049   0.19600584 -0.06529497]\n",
            " [-0.4294049   0.19600584 -0.06529497 -0.11941829 -0.21561317  0.47800217]\n",
            " [-0.11941829 -0.21561317  0.47800217 -0.3047382   0.3998854   0.03973776]\n",
            " [-0.11941829 -0.21561317  0.47800217  0.57624235  0.5029172  -0.31225734]\n",
            " [-0.11941829 -0.21561317  0.47800217 -0.4294049   0.19600584 -0.06529497]\n",
            " [-0.11941829 -0.21561317  0.47800217 -0.11941829 -0.21561317  0.47800217]]\n",
            "\n",
            "\n",
            "----- Attention coefficients. Shape(1, len(emb.concat(emb)))\n",
            "\n",
            "[[0.09834683 0.42110763 0.95788953 0.53316528 0.69187711 0.31551563]]\n",
            "\n",
            "\n",
            "----- Edge representations combined with the attention coefficients. Shape(1, number of edges)\n",
            "\n",
            "[[ 0.30322275]\n",
            " [ 0.73315639]\n",
            " [ 0.11150219]\n",
            " [ 0.11445879]\n",
            " [ 0.09607946]\n",
            " [ 0.52601309]\n",
            " [-0.0956411 ]\n",
            " [-0.14458757]\n",
            " [-0.0926845 ]\n",
            " [ 0.07860653]\n",
            " [ 0.50854017]\n",
            " [-0.11311402]\n",
            " [-0.16206049]\n",
            " [ 0.53443082]\n",
            " [-0.08722337]\n",
            " [-0.13616985]\n",
            " [-0.08426678]\n",
            " [ 0.48206613]\n",
            " [ 0.91199976]\n",
            " [ 0.2413991 ]\n",
            " [ 0.29330217]]\n",
            "\n",
            "\n",
            "----- Leaky Relu. Shape(1, number of edges)\n",
            "[[ 3.03222751e-01]\n",
            " [ 7.33156386e-01]\n",
            " [ 1.11502195e-01]\n",
            " [ 1.14458791e-01]\n",
            " [ 9.60794571e-02]\n",
            " [ 5.26013092e-01]\n",
            " [-9.56410988e-04]\n",
            " [-1.44587571e-03]\n",
            " [-9.26845030e-04]\n",
            " [ 7.86065337e-02]\n",
            " [ 5.08540169e-01]\n",
            " [-1.13114022e-03]\n",
            " [-1.62060495e-03]\n",
            " [ 5.34430817e-01]\n",
            " [-8.72233739e-04]\n",
            " [-1.36169846e-03]\n",
            " [-8.42667781e-04]\n",
            " [ 4.82066128e-01]\n",
            " [ 9.11999763e-01]\n",
            " [ 2.41399100e-01]\n",
            " [ 2.93302168e-01]]\n"
          ]
        }
      ],
      "source": [
        "# equation (2)\n",
        "print('\\n\\n----- Concat hidden features to represent edges. Shape(len(emb.concat(emb)), number of edges)\\n')\n",
        "edge_coords = np.where(A==1)\n",
        "h_src_nodes = z1[edge_coords[0]]\n",
        "h_dst_nodes = z1[edge_coords[1]]\n",
        "z2 = np.concatenate((h_src_nodes, h_dst_nodes), axis=1)\n",
        "\n",
        "# Concatenation tests\n",
        "assert len(edge_coords[1]) == z2.shape[0], \"The number of edges in A is not equal to the number of concat edges\"\n",
        "test_value = np.array([-0.11941829, -0.12942953, 0.19600584, 0.5029172, 0.3998854, -0.21561317])\n",
        "assert z2[4 ,:].tolist().sort()  == test_value.tolist().sort(), \"Something went wrong in the concat process\"\n",
        "print(z2)\n",
        "\n",
        "print('\\n\\n----- Attention coefficients. Shape(1, len(emb.concat(emb)))\\n')\n",
        "att = np.random.rand(1, z2.shape[1])\n",
        "print(att)\n",
        "\n",
        "print('\\n\\n----- Edge representations combined with the attention coefficients. Shape(1, number of edges)\\n')\n",
        "z2_att = z2.dot(att.T)\n",
        "print(z2_att)\n",
        "\n",
        "print('\\n\\n----- Leaky Relu. Shape(1, number of edges)')\n",
        "e = leaky_relu(z2_att)\n",
        "print(e)"
      ]
    },
    {
      "source": [
        "### Normalize the Attention Scores"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "GqVIflB8MqVR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "62hyW7adMqVS",
        "outputId": "372c4155-342b-4c9e-8832-ad2c0855d88f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "----- Edge scores as matrix. Shape(n,n)\n",
            "\n",
            "[[ 3.03222751e-01  7.33156386e-01  1.11502195e-01  0.00000000e+00\n",
            "   1.14458791e-01]\n",
            " [ 9.60794571e-02  5.26013092e-01 -9.56410988e-04 -1.44587571e-03\n",
            "  -9.26845030e-04]\n",
            " [ 7.86065337e-02  5.08540169e-01 -1.13114022e-03 -1.62060495e-03\n",
            "   0.00000000e+00]\n",
            " [ 0.00000000e+00  5.34430817e-01 -8.72233739e-04 -1.36169846e-03\n",
            "  -8.42667781e-04]\n",
            " [ 4.82066128e-01  9.11999763e-01  0.00000000e+00  2.41399100e-01\n",
            "   2.93302168e-01]]\n",
            "\n",
            "\n",
            "----- For each node, normalize the edge (or neighbor) contributions using softmax\n",
            "\n",
            "[0.26263543 0.21349717 0.20979916 0.31406823 0.21610715 0.17567419\n",
            " 0.1726313  0.1771592  0.25842816 0.27167844 0.24278118 0.24273876\n",
            " 0.24280162 0.23393014 0.23388927 0.23394984 0.29823075 0.25138555\n",
            " 0.22399017 0.22400903 0.30061525]\n",
            "\n",
            "\n",
            "----- Normalized edge score matrix. Shape(n,n)\n",
            "\n",
            "[[0.26263543 0.21349717 0.20979916 0.         0.31406823]\n",
            " [0.21610715 0.17567419 0.1726313  0.1771592  0.25842816]\n",
            " [0.27167844 0.24278118 0.24273876 0.24280162 0.        ]\n",
            " [0.         0.23393014 0.23388927 0.23394984 0.29823075]\n",
            " [0.25138555 0.22399017 0.         0.22400903 0.30061525]]\n"
          ]
        }
      ],
      "source": [
        "# equation (3)\n",
        "print('\\n\\n----- Edge scores as matrix. Shape(n,n)\\n')\n",
        "e_matr = np.zeros(A.shape)\n",
        "e_matr[edge_coords[0], edge_coords[1]] = e.reshape(-1,)\n",
        "print(e_matr)\n",
        "\n",
        "print('\\n\\n----- For each node, normalize the edge (or neighbor) contributions using softmax\\n')\n",
        "alpha0 = softmax(e_matr[:,0][e_matr[:,0] != 0]) \n",
        "alpha1 = softmax(e_matr[:,1][e_matr[:,1] != 0])\n",
        "alpha2 = softmax(e_matr[:,2][e_matr[:,2] != 0])\n",
        "alpha3 = softmax(e_matr[:,3][e_matr[:,3] != 0])\n",
        "alpha4 = softmax(e_matr[:,4][e_matr[:,4] != 0])\n",
        "alpha = np.concatenate((alpha0, alpha1, alpha2, alpha3, alpha4))\n",
        "print(alpha)\n",
        "\n",
        "print('\\n\\n----- Normalized edge score matrix. Shape(n,n)\\n')\n",
        "A_scaled = np.zeros(A.shape)\n",
        "A_scaled[edge_coords[0], edge_coords[1]] = alpha.reshape(-1,)\n",
        "print(A_scaled)"
      ]
    },
    {
      "source": [
        "### Neighborhood Diffusion (GCN) Scaled by the Attention Scores (GAT)"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "uXLlQH5zMqVT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wUf-QMQpMqVU",
        "outputId": "452e5a67-91be-45e0-e920-c7cbb0d27f9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Neighborhood aggregation (GCN) scaled with attention scores (GAT). Shape(n, emb)\n",
            "\n",
            "[[-0.02166863  0.15062515  0.08352843]\n",
            " [-0.09390287  0.15866476  0.05716299]\n",
            " [-0.07856777  0.28521023 -0.09286313]\n",
            " [-0.03154513  0.10583032  0.04267501]\n",
            " [-0.07962369  0.19226439  0.069115  ]]\n"
          ]
        }
      ],
      "source": [
        "# equation (4)\n",
        "print('\\n\\nNeighborhood aggregation (GCN) scaled with attention scores (GAT). Shape(n, emb)\\n')\n",
        "ND_GAT = A_scaled.dot(z1)\n",
        "print(ND_GAT)"
      ]
    },
    {
      "source": [
        "## GAT Layer - DGL Test\n",
        "Original layer implementation: https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html  "
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "rsh51TxaMqVV"
      }
    },
    {
      "source": [
        "class GATTestLayer(nn.Module):\n",
        "    def __init__(self, g, in_dim, out_dim):\n",
        "        super(GATTestLayer, self).__init__()\n",
        "        self.g = g\n",
        "        # equation (1)\n",
        "        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
        "        # equation (2)\n",
        "        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"Reinizialitation modified for testing\"\"\"\n",
        "        gain = nn.init.calculate_gain('relu')\n",
        "        self.fc.state_dict()['weight'][:] = torch.from_numpy(W)\n",
        "        self.attn_fc.state_dict()['weight'][:] = torch.from_numpy(att)\n",
        "\n",
        "    def edge_attention(self, edges):\n",
        "        # edge UDF for equation (2)\n",
        "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
        "        a = self.attn_fc(z2)\n",
        "        return {'e': F.leaky_relu(a)}\n",
        "\n",
        "    def message_func(self, edges):\n",
        "        # message UDF for equation (3) & (4)\n",
        "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
        "\n",
        "    def reduce_func(self, nodes):\n",
        "        # reduce UDF for equation (3) & (4)\n",
        "        # equation (3)\n",
        "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
        "        # equation (4)\n",
        "        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
        "        return {'h': h}\n",
        "\n",
        "    def forward(self, h):\n",
        "        # equation (1)\n",
        "        z = self.fc(h)\n",
        "        self.g.ndata['z'] = z\n",
        "        # equation (2)\n",
        "        self.g.apply_edges(self.edge_attention)\n",
        "        # equation (3) & (4)\n",
        "        self.g.update_all(self.message_func, self.reduce_func)\n",
        "        return self.g.ndata.pop('h')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "UucL4aXPMqVV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3aX8szuVMqVW",
        "outputId": "9184e7ce-da95-47a5-8f2e-fa7c22b0e44e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "----- Create a new DGL graph using the NumPy graph\n",
            "\n",
            "Graph(num_nodes=5, num_edges=21,\n",
            "      ndata_schemes={}\n",
            "      edata_schemes={})\n",
            "\n",
            "\n",
            "----- Create a DGL instance of the GAT test layer\n",
            "\n",
            "tensor([[-0.0217,  0.1506,  0.0835],\n",
            "        [-0.0939,  0.1587,  0.0572],\n",
            "        [-0.0786,  0.2852, -0.0929],\n",
            "        [-0.0315,  0.1058,  0.0427],\n",
            "        [-0.0796,  0.1923,  0.0691]], grad_fn=<IndexCopyBackward0>)\n",
            "\n",
            "\n",
            "----- Recap of the NumPy GAT layer\n",
            "[[-0.0217  0.1506  0.0835]\n",
            " [-0.0939  0.1587  0.0572]\n",
            " [-0.0786  0.2852 -0.0929]\n",
            " [-0.0315  0.1058  0.0427]\n",
            " [-0.0796  0.1923  0.0691]]\n"
          ]
        }
      ],
      "source": [
        "print('\\n\\n----- Create a new DGL graph using the NumPy graph\\n')\n",
        "src_ids = torch.tensor(edge_coords[0])\n",
        "dst_ids = torch.tensor(edge_coords[1])\n",
        "g = dgl.graph((src_ids, dst_ids))\n",
        "print(g)\n",
        "\n",
        "print('\\n\\n----- Create a DGL instance of the GAT test layer\\n')\n",
        "net = GATTestLayer(g,\n",
        "          in_dim=n,\n",
        "          out_dim=3)\n",
        "print(net.forward(torch.Tensor(X)))\n",
        "\n",
        "print('\\n\\n----- Recap of the NumPy GAT layer')\n",
        "print(np.round(ND_GAT, decimals=4))\n",
        "\n"
      ]
    },
    {
      "source": [
        "The resulting matrices from the NumPy implementation and the DGL implementation are equal \\o/."
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "QHn-WeLrMqVW"
      }
    },
    {
      "source": [
        "## Multi Head GAT Layer Implementation with NumPy\n",
        "Multiple head attentions are created generating multiple GAT layers."
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "Es2W8fYZMqVX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9mGFuYCkMqVX",
        "outputId": "bacf05d1-756a-4cf1-d803-8ac32a2e098a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "----- Recap on the output of the GAT layer\n",
            "\n",
            "Layer 1. Shape(emb,n)\n",
            "[[-0.02166863  0.15062515  0.08352843]\n",
            " [-0.09390287  0.15866476  0.05716299]\n",
            " [-0.07856777  0.28521023 -0.09286313]\n",
            " [-0.03154513  0.10583032  0.04267501]\n",
            " [-0.07962369  0.19226439  0.069115  ]]\n",
            "\n",
            "Layer 2. Shape(emb,n)\n",
            "[[-0.02166863  0.15062515  0.08352843]\n",
            " [-0.09390287  0.15866476  0.05716299]\n",
            " [-0.07856777  0.28521023 -0.09286313]\n",
            " [-0.03154513  0.10583032  0.04267501]\n",
            " [-0.07962369  0.19226439  0.069115  ]]\n",
            "\n",
            "\n",
            "----- Concatenate multiple attentions. Shape(num_layers*emb, n)\n",
            "\n",
            "[[-0.02166863  0.15062515  0.08352843 -0.02166863  0.15062515  0.08352843]\n",
            " [-0.09390287  0.15866476  0.05716299 -0.09390287  0.15866476  0.05716299]\n",
            " [-0.07856777  0.28521023 -0.09286313 -0.07856777  0.28521023 -0.09286313]\n",
            " [-0.03154513  0.10583032  0.04267501 -0.03154513  0.10583032  0.04267501]\n",
            " [-0.07962369  0.19226439  0.069115   -0.07962369  0.19226439  0.069115  ]]\n",
            "\n",
            "\n",
            "----- Average multiple attentions.\n",
            "\n",
            "0.04979367027023359\n"
          ]
        }
      ],
      "source": [
        "print('\\n\\n----- Recap on the output of the GAT layer')\n",
        "print('\\nLayer 1. Shape(emb,n)')\n",
        "layer1 = ND_GAT\n",
        "print(layer1)\n",
        "\n",
        "print('\\nLayer 2. Shape(emb,n)')\n",
        "layer2 = ND_GAT\n",
        "print(layer2)\n",
        "\n",
        "print('\\n\\n----- Concatenate multiple attentions. Shape(num_layers*emb, n)\\n')\n",
        "concat = np.concatenate((layer1, layer2), axis=1)\n",
        "print(concat)\n",
        "\n",
        "print('\\n\\n----- Average multiple attentions.\\n')\n",
        "# 30 is the number of parameters: num_layers*emb*n\n",
        "average = np.sum((layer1, layer2)) / 30\n",
        "print(average)"
      ]
    },
    {
      "source": [
        "## Multi Head GAT Layer - DGL Test\n",
        "Original layer implementation: https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html  "
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "oZwaztq6MqVY"
      }
    },
    {
      "source": [
        "class MultiHeadGATTestLayer(nn.Module):\n",
        "    def __init__(self, g, in_dim, out_dim, num_heads, merge='cat'):\n",
        "        super(MultiHeadGATTestLayer, self).__init__()\n",
        "        self.heads = nn.ModuleList()\n",
        "        for i in range(num_heads):\n",
        "            # Use the test layer for consistency with the NumPy implementation\n",
        "            self.heads.append(GATTestLayer(g, in_dim, out_dim))\n",
        "        self.merge = merge\n",
        "\n",
        "    def forward(self, h):\n",
        "        head_outs = [attn_head(h) for attn_head in self.heads]\n",
        "        if self.merge == 'cat':\n",
        "            # concat on the output feature dimension (dim=1)\n",
        "            return torch.cat(head_outs, dim=1)\n",
        "        else:\n",
        "            # merge using average\n",
        "            return torch.mean(torch.stack(head_outs))\n",
        "\n",
        "print('\\n\\n----- Multi head GAT layer (concat operation). Shape(num_layers*emb, n)\\n')\n",
        "concat_net = MultiHeadGATTestLayer(g, in_dim=n, out_dim=3, num_heads=2)\n",
        "print(concat_net)\n",
        "print('\\n----- DGL concat output\\n')\n",
        "print(concat_net.forward(torch.Tensor(X)))\n",
        "\n",
        "print('\\n----- Recap of the NumPy concatenation\\n')\n",
        "print(np.round(concat, decimals=4))\n",
        "\n",
        "print('\\n\\n----- Multi head GAT Layer (average operation). Shape(emb, n)\\n')\n",
        "mean_net = MultiHeadGATTestLayer(g, in_dim=n, out_dim=3, num_heads=2, merge='mean')\n",
        "print(mean_net)\n",
        "print('\\n----- DGL average output\\n')\n",
        "print(mean_net.forward(torch.Tensor(X)))\n",
        "\n",
        "print('\\n----- Recap of the NumPy average\\n')\n",
        "print(np.round(average, decimals=4))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mmLIkmB5MqVY",
        "outputId": "52567e71-e031-45df-ebe4-9ac6ec16265d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "----- Multi head GAT layer (concat operation). Shape(num_layers*emb, n)\n",
            "\n",
            "MultiHeadGATTestLayer(\n",
            "  (heads): ModuleList(\n",
            "    (0): GATTestLayer(\n",
            "      (fc): Linear(in_features=5, out_features=3, bias=False)\n",
            "      (attn_fc): Linear(in_features=6, out_features=1, bias=False)\n",
            "    )\n",
            "    (1): GATTestLayer(\n",
            "      (fc): Linear(in_features=5, out_features=3, bias=False)\n",
            "      (attn_fc): Linear(in_features=6, out_features=1, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\n",
            "----- DGL concat output\n",
            "\n",
            "tensor([[-0.0217,  0.1506,  0.0835, -0.0217,  0.1506,  0.0835],\n",
            "        [-0.0939,  0.1587,  0.0572, -0.0939,  0.1587,  0.0572],\n",
            "        [-0.0786,  0.2852, -0.0929, -0.0786,  0.2852, -0.0929],\n",
            "        [-0.0315,  0.1058,  0.0427, -0.0315,  0.1058,  0.0427],\n",
            "        [-0.0796,  0.1923,  0.0691, -0.0796,  0.1923,  0.0691]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "\n",
            "----- Recap of the NumPy concatenation\n",
            "\n",
            "[[-0.0217  0.1506  0.0835 -0.0217  0.1506  0.0835]\n",
            " [-0.0939  0.1587  0.0572 -0.0939  0.1587  0.0572]\n",
            " [-0.0786  0.2852 -0.0929 -0.0786  0.2852 -0.0929]\n",
            " [-0.0315  0.1058  0.0427 -0.0315  0.1058  0.0427]\n",
            " [-0.0796  0.1923  0.0691 -0.0796  0.1923  0.0691]]\n",
            "\n",
            "\n",
            "----- Multi head GAT Layer (average operation). Shape(emb, n)\n",
            "\n",
            "MultiHeadGATTestLayer(\n",
            "  (heads): ModuleList(\n",
            "    (0): GATTestLayer(\n",
            "      (fc): Linear(in_features=5, out_features=3, bias=False)\n",
            "      (attn_fc): Linear(in_features=6, out_features=1, bias=False)\n",
            "    )\n",
            "    (1): GATTestLayer(\n",
            "      (fc): Linear(in_features=5, out_features=3, bias=False)\n",
            "      (attn_fc): Linear(in_features=6, out_features=1, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\n",
            "----- DGL average output\n",
            "\n",
            "tensor(0.0498, grad_fn=<MeanBackward0>)\n",
            "\n",
            "----- Recap of the NumPy average\n",
            "\n",
            "0.0498\n"
          ]
        }
      ]
    },
    {
      "source": [
        "The resulting matrices from the NumPy implementation and the DGL implementation are equal \\o/."
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "fKqgPYKbMqVZ"
      }
    },
    {
      "source": [
        "# From Theory to Practice\n",
        "After the understanding of math and the implementation of GAT building blocks, we can run some experiments as reported in the original paper. Let's recap the DGL modules using a fair parameter initialization. The following implementation is based on the example available here: https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html."
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "0q-yhcuvMqVZ"
      }
    },
    {
      "source": [
        "## New Imports"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "0KGZqkOHMqVa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0fMvJS5-MqVa"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from dgl import DGLGraph\n",
        "from dgl.data import citation_graph as citegrh\n",
        "import networkx as nx"
      ]
    },
    {
      "source": [
        "## GAT Implementation with DGL"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "66a1truZMqVa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "so_0nsCCMqVb"
      },
      "outputs": [],
      "source": [
        "class GATLayer(nn.Module):\n",
        "    def __init__(self, g, in_dim, out_dim):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.g = g\n",
        "        # equation (1)\n",
        "        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
        "        # equation (2)\n",
        "        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
        "        gain = nn.init.calculate_gain('relu')\n",
        "        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n",
        "\n",
        "    def edge_attention(self, edges):\n",
        "        # edge UDF for equation (2)\n",
        "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
        "        a = self.attn_fc(z2)\n",
        "        return {'e': F.leaky_relu(a)}\n",
        "\n",
        "    def message_func(self, edges):\n",
        "        # message UDF for equation (3) & (4)\n",
        "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
        "\n",
        "    def reduce_func(self, nodes):\n",
        "        # reduce UDF for equation (3) & (4)\n",
        "        # equation (3)\n",
        "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
        "        # equation (4)\n",
        "        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
        "        return {'h': h}\n",
        "\n",
        "    def forward(self, h):\n",
        "        # equation (1)\n",
        "        z = self.fc(h)\n",
        "        self.g.ndata['z'] = z\n",
        "        # equation (2)\n",
        "        self.g.apply_edges(self.edge_attention)\n",
        "        # equation (3) & (4)\n",
        "        self.g.update_all(self.message_func, self.reduce_func)\n",
        "        return self.g.ndata.pop('h')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rXYCOVnzMqVc"
      },
      "outputs": [],
      "source": [
        "class MultiHeadGATLayer(nn.Module):\n",
        "    def __init__(self, g, in_dim, out_dim, num_heads, merge='cat'):\n",
        "        super(MultiHeadGATLayer, self).__init__()\n",
        "        self.heads = nn.ModuleList()\n",
        "        for i in range(num_heads):\n",
        "            self.heads.append(GATLayer(g, in_dim, out_dim))\n",
        "        self.merge = merge\n",
        "\n",
        "    def forward(self, h):\n",
        "        head_outs = [attn_head(h) for attn_head in self.heads]\n",
        "        if self.merge == 'cat':\n",
        "            # concat on the output feature dimension (dim=1)\n",
        "            return torch.cat(head_outs, dim=1)\n",
        "        else:\n",
        "            # merge using average\n",
        "            return torch.mean(torch.stack(head_outs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qYgULP7fMqVc"
      },
      "outputs": [],
      "source": [
        "class GAT(nn.Module):\n",
        "    def __init__(self, g, in_dim, hidden_dim, out_dim, num_heads):\n",
        "        super(GAT, self).__init__()\n",
        "        self.layer1 = MultiHeadGATLayer(g, in_dim, hidden_dim, num_heads)\n",
        "        # Be aware that the input dimension is hidden_dim*num_heads since\n",
        "        # multiple head outputs are concatenated together. Also, only\n",
        "        # one attention head in the output layer.\n",
        "        self.layer2 = MultiHeadGATLayer(g, hidden_dim * num_heads, out_dim, 1)\n",
        "\n",
        "    def forward(self, h):\n",
        "        h = self.layer1(h)\n",
        "        h = F.elu(h)\n",
        "        h = self.layer2(h)\n",
        "        return h"
      ]
    },
    {
      "source": [
        "## Evaluation Functions"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "M7rSAvDeMqVc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BfrH_zlZMqVd"
      },
      "outputs": [],
      "source": [
        "def accuracy(logits, labels):\n",
        "    _, indices = torch.max(logits, dim=1)\n",
        "    correct = torch.sum(indices == labels)\n",
        "    return correct.item() * 1.0 / len(labels)\n",
        "\n",
        "def evaluate(model, features, labels, mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(features)\n",
        "        logits = logits[mask]\n",
        "        labels = labels[mask]\n",
        "        return accuracy(logits, labels)"
      ]
    },
    {
      "source": [
        "## Load Cora Dataset"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "kmgWRJ-gMqVd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-LmqqkUUMqVd",
        "outputId": "56bd70fe-1abe-445c-bf90-041b00c9fb9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading /root/.dgl/cora_v2.zip from https://data.dgl.ai/dataset/cora_v2.zip...\n",
            "Extracting file to /root/.dgl/cora_v2\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done saving data into cached files.\n",
            "Dataset(\"cora_v2\", num_graphs=1, save_path=/root/.dgl/cora_v2)\n",
            "\n",
            "\n",
            "----- Features of CORA dataset\n",
            "\n",
            "----- Graph:\n",
            "Graph(num_nodes=2708, num_edges=10556,\n",
            "      ndata_schemes={'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool), 'label': Scheme(shape=(), dtype=torch.int64), 'feat': Scheme(shape=(1433,), dtype=torch.float32)}\n",
            "      edata_schemes={})\n",
            "\n",
            "----- Features:\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "tensor([  19,   81,  146,  ..., 1328, 1412, 1414])\n",
            "\n",
            "----- Labels:\n",
            "tensor([3, 4, 4,  ..., 3, 3, 3])\n",
            "torch.Size([2708])\n",
            "----- Number of unique labels:\n",
            "tensor([0, 1, 2, 3, 4, 5, 6])\n",
            "----- Number of label occurrences:\n",
            "tensor([351, 217, 418, 818, 426, 298, 180])\n",
            "\n",
            "----- Training mask:\n",
            "tensor([0, 1, 2, 3, 4, 5, 6])\n",
            "tensor([2568,  140])\n",
            "\n",
            "----- Validation mask:\n",
            "tensor([0, 1, 2, 3, 4, 5, 6])\n",
            "tensor([2208,  500])\n",
            "\n",
            "----- Testing mask:\n",
            "tensor([0, 1, 2, 3, 4, 5, 6])\n",
            "tensor([1708, 1000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/dgl/data/utils.py:288: UserWarning: Property dataset.feat will be deprecated, please use g.ndata['feat'] instead.\n",
            "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
            "/usr/local/lib/python3.7/dist-packages/dgl/data/utils.py:288: UserWarning: Property dataset.label will be deprecated, please use g.ndata['label'] instead.\n",
            "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
            "/usr/local/lib/python3.7/dist-packages/dgl/data/utils.py:288: UserWarning: Property dataset.train_mask will be deprecated, please use g.ndata['train_mask'] instead.\n",
            "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
            "/usr/local/lib/python3.7/dist-packages/dgl/data/utils.py:288: UserWarning: Property dataset.val_mask will be deprecated, please use g.ndata['val_mask'] instead.\n",
            "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
            "/usr/local/lib/python3.7/dist-packages/dgl/data/utils.py:288: UserWarning: Property dataset.test_mask will be deprecated, please use g.ndata['test_mask'] instead.\n",
            "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n"
          ]
        }
      ],
      "source": [
        "def load_cora_data():\n",
        "    data = citegrh.load_cora()\n",
        "    features = torch.FloatTensor(data.features)\n",
        "    labels = torch.LongTensor(data.labels)\n",
        "    train_mask = torch.BoolTensor(data.train_mask)\n",
        "    val_mask = torch.BoolTensor(data.val_mask)\n",
        "    test_mask = torch.BoolTensor(data.test_mask)\n",
        "    print(data)\n",
        "    graph=data[0]\n",
        "    g = graph\n",
        "    return g, features, labels, train_mask, val_mask, test_mask\n",
        "\n",
        "g, features, labels, train_mask, val_mask, test_mask = load_cora_data()\n",
        "print('\\n\\n----- Features of CORA dataset')\n",
        "\n",
        "print('\\n----- Graph:')\n",
        "print(g)\n",
        "\n",
        "print('\\n----- Features:')\n",
        "print(features)\n",
        "print(features.nonzero(as_tuple=True)[1])\n",
        "\n",
        "print('\\n----- Labels:')\n",
        "print(labels)\n",
        "print(labels.size())\n",
        "output = torch.unique(labels)\n",
        "occs = torch.bincount(labels)\n",
        "print('----- Number of unique labels:')\n",
        "print(output)\n",
        "print('----- Number of label occurrences:')\n",
        "print(occs)\n",
        "\n",
        "print('\\n----- Training mask:')\n",
        "train_long = train_mask.long()\n",
        "occs = torch.bincount(train_long)\n",
        "print(output)\n",
        "print(occs)\n",
        "\n",
        "print('\\n----- Validation mask:')\n",
        "val_long = val_mask.long()\n",
        "occs = torch.bincount(val_long)\n",
        "print(output)\n",
        "print(occs)\n",
        "\n",
        "print('\\n----- Testing mask:')\n",
        "test_long = test_mask.long()\n",
        "occs = torch.bincount(test_long)\n",
        "print(output)\n",
        "print(occs)\n"
      ]
    },
    {
      "source": [
        "Analyzing the cora dataset, you can get the following information:\n",
        "\n",
        "1. Nodes have no features (one-hot encoding vectors)\n",
        "2. Node labels are uniformly distributed\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "JveGR3NwMqVe"
      }
    },
    {
      "source": [
        "## Training Loop"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "wJslrq70MqVe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "X7ZtpExkMqVe",
        "outputId": "b0468262-1158-435f-b21b-9251826b2fee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00000 | Time(s) 0.3260 | Loss 1.9456 | Training Accuracy 0.1357\n",
            "\n",
            "Eval on validation dataset...\n",
            "Validation Accuracy: 0.1760\n",
            "\n",
            "Epoch 00001 | Time(s) 0.2356 | Loss 1.9437 | Training Accuracy 0.1857\n",
            "Epoch 00002 | Time(s) 0.2043 | Loss 1.9418 | Training Accuracy 0.2500\n",
            "Epoch 00003 | Time(s) 0.1967 | Loss 1.9398 | Training Accuracy 0.3214\n",
            "Epoch 00004 | Time(s) 0.1867 | Loss 1.9379 | Training Accuracy 0.3929\n",
            "Epoch 00005 | Time(s) 0.1817 | Loss 1.9359 | Training Accuracy 0.4286\n",
            "Epoch 00006 | Time(s) 0.1936 | Loss 1.9340 | Training Accuracy 0.4857\n",
            "Epoch 00007 | Time(s) 0.1954 | Loss 1.9320 | Training Accuracy 0.5500\n",
            "Epoch 00008 | Time(s) 0.1886 | Loss 1.9301 | Training Accuracy 0.6143\n",
            "Epoch 00009 | Time(s) 0.1832 | Loss 1.9281 | Training Accuracy 0.6571\n",
            "Epoch 00010 | Time(s) 0.1795 | Loss 1.9262 | Training Accuracy 0.6857\n",
            "Epoch 00011 | Time(s) 0.1758 | Loss 1.9242 | Training Accuracy 0.7643\n",
            "Epoch 00012 | Time(s) 0.1737 | Loss 1.9222 | Training Accuracy 0.7929\n",
            "Epoch 00013 | Time(s) 0.1722 | Loss 1.9202 | Training Accuracy 0.8000\n",
            "Epoch 00014 | Time(s) 0.1709 | Loss 1.9182 | Training Accuracy 0.8143\n",
            "Epoch 00015 | Time(s) 0.1698 | Loss 1.9162 | Training Accuracy 0.8500\n",
            "Epoch 00016 | Time(s) 0.1688 | Loss 1.9142 | Training Accuracy 0.8571\n",
            "Epoch 00017 | Time(s) 0.1679 | Loss 1.9122 | Training Accuracy 0.8571\n",
            "Epoch 00018 | Time(s) 0.1672 | Loss 1.9101 | Training Accuracy 0.8786\n",
            "Epoch 00019 | Time(s) 0.1660 | Loss 1.9081 | Training Accuracy 0.9071\n",
            "Epoch 00020 | Time(s) 0.1645 | Loss 1.9060 | Training Accuracy 0.9214\n",
            "Epoch 00021 | Time(s) 0.1637 | Loss 1.9040 | Training Accuracy 0.9357\n",
            "Epoch 00022 | Time(s) 0.1623 | Loss 1.9019 | Training Accuracy 0.9500\n",
            "Epoch 00023 | Time(s) 0.1614 | Loss 1.8998 | Training Accuracy 0.9500\n",
            "Epoch 00024 | Time(s) 0.1601 | Loss 1.8977 | Training Accuracy 0.9571\n",
            "Epoch 00025 | Time(s) 0.1594 | Loss 1.8956 | Training Accuracy 0.9571\n",
            "Epoch 00026 | Time(s) 0.1587 | Loss 1.8934 | Training Accuracy 0.9571\n",
            "Epoch 00027 | Time(s) 0.1576 | Loss 1.8913 | Training Accuracy 0.9643\n",
            "Epoch 00028 | Time(s) 0.1568 | Loss 1.8891 | Training Accuracy 0.9714\n",
            "Epoch 00029 | Time(s) 0.1559 | Loss 1.8870 | Training Accuracy 0.9714\n",
            "Epoch 00030 | Time(s) 0.1621 | Loss 1.8848 | Training Accuracy 0.9714\n",
            "\n",
            "Eval on validation dataset...\n",
            "Validation Accuracy: 0.7060\n",
            "\n",
            "Epoch 00031 | Time(s) 0.1661 | Loss 1.8826 | Training Accuracy 0.9643\n",
            "Epoch 00032 | Time(s) 0.1693 | Loss 1.8804 | Training Accuracy 0.9643\n",
            "Epoch 00033 | Time(s) 0.1723 | Loss 1.8781 | Training Accuracy 0.9571\n",
            "Epoch 00034 | Time(s) 0.1759 | Loss 1.8759 | Training Accuracy 0.9571\n",
            "Epoch 00035 | Time(s) 0.1782 | Loss 1.8736 | Training Accuracy 0.9643\n",
            "Epoch 00036 | Time(s) 0.1838 | Loss 1.8714 | Training Accuracy 0.9643\n",
            "Epoch 00037 | Time(s) 0.1889 | Loss 1.8691 | Training Accuracy 0.9714\n",
            "Epoch 00038 | Time(s) 0.1915 | Loss 1.8668 | Training Accuracy 0.9714\n",
            "Epoch 00039 | Time(s) 0.1958 | Loss 1.8645 | Training Accuracy 0.9714\n",
            "Epoch 00040 | Time(s) 0.2005 | Loss 1.8621 | Training Accuracy 0.9714\n",
            "Epoch 00041 | Time(s) 0.2031 | Loss 1.8598 | Training Accuracy 0.9714\n",
            "Epoch 00042 | Time(s) 0.2062 | Loss 1.8574 | Training Accuracy 0.9714\n",
            "Epoch 00043 | Time(s) 0.2073 | Loss 1.8550 | Training Accuracy 0.9714\n",
            "Epoch 00044 | Time(s) 0.2098 | Loss 1.8527 | Training Accuracy 0.9714\n",
            "Epoch 00045 | Time(s) 0.2110 | Loss 1.8502 | Training Accuracy 0.9714\n",
            "Epoch 00046 | Time(s) 0.2133 | Loss 1.8478 | Training Accuracy 0.9714\n",
            "Epoch 00047 | Time(s) 0.2166 | Loss 1.8454 | Training Accuracy 0.9714\n",
            "Epoch 00048 | Time(s) 0.2178 | Loss 1.8429 | Training Accuracy 0.9714\n",
            "Epoch 00049 | Time(s) 0.2200 | Loss 1.8404 | Training Accuracy 0.9714\n",
            "Epoch 00050 | Time(s) 0.2206 | Loss 1.8379 | Training Accuracy 0.9714\n",
            "Epoch 00051 | Time(s) 0.2220 | Loss 1.8354 | Training Accuracy 0.9714\n",
            "Epoch 00052 | Time(s) 0.2247 | Loss 1.8329 | Training Accuracy 0.9714\n",
            "Epoch 00053 | Time(s) 0.2289 | Loss 1.8304 | Training Accuracy 0.9714\n",
            "Epoch 00054 | Time(s) 0.2337 | Loss 1.8278 | Training Accuracy 0.9714\n",
            "Epoch 00055 | Time(s) 0.2348 | Loss 1.8252 | Training Accuracy 0.9714\n",
            "Epoch 00056 | Time(s) 0.2355 | Loss 1.8226 | Training Accuracy 0.9714\n",
            "Epoch 00057 | Time(s) 0.2377 | Loss 1.8200 | Training Accuracy 0.9714\n",
            "Epoch 00058 | Time(s) 0.2385 | Loss 1.8174 | Training Accuracy 0.9714\n",
            "Epoch 00059 | Time(s) 0.2400 | Loss 1.8148 | Training Accuracy 0.9714\n",
            "Epoch 00060 | Time(s) 0.2422 | Loss 1.8121 | Training Accuracy 0.9714\n",
            "\n",
            "Eval on validation dataset...\n",
            "Validation Accuracy: 0.7320\n",
            "\n",
            "Epoch 00061 | Time(s) 0.2431 | Loss 1.8094 | Training Accuracy 0.9714\n",
            "Epoch 00062 | Time(s) 0.2430 | Loss 1.8067 | Training Accuracy 0.9714\n",
            "Epoch 00063 | Time(s) 0.2427 | Loss 1.8040 | Training Accuracy 0.9714\n",
            "Epoch 00064 | Time(s) 0.2451 | Loss 1.8013 | Training Accuracy 0.9714\n",
            "Epoch 00065 | Time(s) 0.2466 | Loss 1.7986 | Training Accuracy 0.9714\n",
            "Epoch 00066 | Time(s) 0.2470 | Loss 1.7958 | Training Accuracy 0.9714\n",
            "Epoch 00067 | Time(s) 0.2482 | Loss 1.7930 | Training Accuracy 0.9714\n",
            "Epoch 00068 | Time(s) 0.2487 | Loss 1.7902 | Training Accuracy 0.9714\n",
            "Epoch 00069 | Time(s) 0.2487 | Loss 1.7874 | Training Accuracy 0.9714\n",
            "Epoch 00070 | Time(s) 0.2492 | Loss 1.7846 | Training Accuracy 0.9714\n",
            "Epoch 00071 | Time(s) 0.2491 | Loss 1.7817 | Training Accuracy 0.9714\n",
            "Epoch 00072 | Time(s) 0.2508 | Loss 1.7789 | Training Accuracy 0.9714\n",
            "Epoch 00073 | Time(s) 0.2512 | Loss 1.7760 | Training Accuracy 0.9714\n",
            "Epoch 00074 | Time(s) 0.2529 | Loss 1.7731 | Training Accuracy 0.9714\n",
            "Epoch 00075 | Time(s) 0.2532 | Loss 1.7702 | Training Accuracy 0.9714\n",
            "Epoch 00076 | Time(s) 0.2530 | Loss 1.7672 | Training Accuracy 0.9714\n",
            "Epoch 00077 | Time(s) 0.2536 | Loss 1.7643 | Training Accuracy 0.9714\n",
            "Epoch 00078 | Time(s) 0.2539 | Loss 1.7613 | Training Accuracy 0.9643\n",
            "Epoch 00079 | Time(s) 0.2554 | Loss 1.7583 | Training Accuracy 0.9643\n",
            "Epoch 00080 | Time(s) 0.2560 | Loss 1.7553 | Training Accuracy 0.9643\n",
            "Epoch 00081 | Time(s) 0.2577 | Loss 1.7523 | Training Accuracy 0.9643\n",
            "Epoch 00082 | Time(s) 0.2576 | Loss 1.7493 | Training Accuracy 0.9643\n",
            "Epoch 00083 | Time(s) 0.2586 | Loss 1.7462 | Training Accuracy 0.9643\n",
            "Epoch 00084 | Time(s) 0.2593 | Loss 1.7432 | Training Accuracy 0.9643\n",
            "Epoch 00085 | Time(s) 0.2593 | Loss 1.7401 | Training Accuracy 0.9643\n",
            "Epoch 00086 | Time(s) 0.2600 | Loss 1.7370 | Training Accuracy 0.9643\n",
            "Epoch 00087 | Time(s) 0.2614 | Loss 1.7339 | Training Accuracy 0.9643\n",
            "Epoch 00088 | Time(s) 0.2637 | Loss 1.7307 | Training Accuracy 0.9643\n",
            "Epoch 00089 | Time(s) 0.2624 | Loss 1.7276 | Training Accuracy 0.9643\n",
            "Epoch 00090 | Time(s) 0.2610 | Loss 1.7244 | Training Accuracy 0.9643\n",
            "\n",
            "Eval on validation dataset...\n",
            "Validation Accuracy: 0.7460\n",
            "\n",
            "Epoch 00091 | Time(s) 0.2596 | Loss 1.7212 | Training Accuracy 0.9643\n",
            "Epoch 00092 | Time(s) 0.2582 | Loss 1.7180 | Training Accuracy 0.9643\n",
            "Epoch 00093 | Time(s) 0.2569 | Loss 1.7148 | Training Accuracy 0.9643\n",
            "Epoch 00094 | Time(s) 0.2555 | Loss 1.7115 | Training Accuracy 0.9643\n",
            "Epoch 00095 | Time(s) 0.2544 | Loss 1.7083 | Training Accuracy 0.9643\n",
            "Epoch 00096 | Time(s) 0.2532 | Loss 1.7050 | Training Accuracy 0.9643\n",
            "Epoch 00097 | Time(s) 0.2519 | Loss 1.7017 | Training Accuracy 0.9643\n",
            "Epoch 00098 | Time(s) 0.2507 | Loss 1.6984 | Training Accuracy 0.9643\n",
            "Epoch 00099 | Time(s) 0.2495 | Loss 1.6951 | Training Accuracy 0.9643\n",
            "Epoch 00100 | Time(s) 0.2485 | Loss 1.6918 | Training Accuracy 0.9643\n",
            "Epoch 00101 | Time(s) 0.2473 | Loss 1.6884 | Training Accuracy 0.9643\n",
            "Epoch 00102 | Time(s) 0.2462 | Loss 1.6850 | Training Accuracy 0.9643\n",
            "Epoch 00103 | Time(s) 0.2452 | Loss 1.6816 | Training Accuracy 0.9714\n",
            "Epoch 00104 | Time(s) 0.2444 | Loss 1.6782 | Training Accuracy 0.9714\n",
            "Epoch 00105 | Time(s) 0.2434 | Loss 1.6748 | Training Accuracy 0.9714\n",
            "Epoch 00106 | Time(s) 0.2427 | Loss 1.6714 | Training Accuracy 0.9714\n",
            "Epoch 00107 | Time(s) 0.2417 | Loss 1.6679 | Training Accuracy 0.9714\n",
            "Epoch 00108 | Time(s) 0.2409 | Loss 1.6645 | Training Accuracy 0.9786\n",
            "Epoch 00109 | Time(s) 0.2399 | Loss 1.6610 | Training Accuracy 0.9786\n",
            "Epoch 00110 | Time(s) 0.2390 | Loss 1.6575 | Training Accuracy 0.9786\n",
            "Epoch 00111 | Time(s) 0.2381 | Loss 1.6539 | Training Accuracy 0.9786\n",
            "Epoch 00112 | Time(s) 0.2371 | Loss 1.6504 | Training Accuracy 0.9786\n",
            "Epoch 00113 | Time(s) 0.2361 | Loss 1.6469 | Training Accuracy 0.9786\n",
            "Epoch 00114 | Time(s) 0.2353 | Loss 1.6433 | Training Accuracy 0.9786\n",
            "Epoch 00115 | Time(s) 0.2344 | Loss 1.6397 | Training Accuracy 0.9786\n",
            "Epoch 00116 | Time(s) 0.2335 | Loss 1.6361 | Training Accuracy 0.9786\n",
            "Epoch 00117 | Time(s) 0.2327 | Loss 1.6325 | Training Accuracy 0.9786\n",
            "Epoch 00118 | Time(s) 0.2318 | Loss 1.6289 | Training Accuracy 0.9786\n",
            "Epoch 00119 | Time(s) 0.2309 | Loss 1.6252 | Training Accuracy 0.9786\n",
            "Epoch 00120 | Time(s) 0.2301 | Loss 1.6216 | Training Accuracy 0.9786\n",
            "\n",
            "Eval on validation dataset...\n",
            "Validation Accuracy: 0.7540\n",
            "\n",
            "Epoch 00121 | Time(s) 0.2294 | Loss 1.6179 | Training Accuracy 0.9786\n",
            "Epoch 00122 | Time(s) 0.2286 | Loss 1.6142 | Training Accuracy 0.9786\n",
            "Epoch 00123 | Time(s) 0.2278 | Loss 1.6105 | Training Accuracy 0.9786\n",
            "Epoch 00124 | Time(s) 0.2272 | Loss 1.6068 | Training Accuracy 0.9786\n",
            "Epoch 00125 | Time(s) 0.2264 | Loss 1.6030 | Training Accuracy 0.9786\n",
            "Epoch 00126 | Time(s) 0.2257 | Loss 1.5993 | Training Accuracy 0.9786\n",
            "Epoch 00127 | Time(s) 0.2250 | Loss 1.5955 | Training Accuracy 0.9786\n",
            "Epoch 00128 | Time(s) 0.2243 | Loss 1.5918 | Training Accuracy 0.9786\n",
            "Epoch 00129 | Time(s) 0.2236 | Loss 1.5880 | Training Accuracy 0.9786\n",
            "Epoch 00130 | Time(s) 0.2229 | Loss 1.5842 | Training Accuracy 0.9786\n",
            "Epoch 00131 | Time(s) 0.2223 | Loss 1.5803 | Training Accuracy 0.9786\n",
            "Epoch 00132 | Time(s) 0.2218 | Loss 1.5765 | Training Accuracy 0.9786\n",
            "Epoch 00133 | Time(s) 0.2212 | Loss 1.5726 | Training Accuracy 0.9786\n",
            "Epoch 00134 | Time(s) 0.2205 | Loss 1.5688 | Training Accuracy 0.9786\n",
            "Epoch 00135 | Time(s) 0.2199 | Loss 1.5649 | Training Accuracy 0.9786\n",
            "Epoch 00136 | Time(s) 0.2193 | Loss 1.5610 | Training Accuracy 0.9786\n",
            "Epoch 00137 | Time(s) 0.2187 | Loss 1.5571 | Training Accuracy 0.9786\n",
            "Epoch 00138 | Time(s) 0.2181 | Loss 1.5532 | Training Accuracy 0.9786\n",
            "Epoch 00139 | Time(s) 0.2175 | Loss 1.5492 | Training Accuracy 0.9786\n",
            "Epoch 00140 | Time(s) 0.2169 | Loss 1.5453 | Training Accuracy 0.9786\n",
            "Epoch 00141 | Time(s) 0.2163 | Loss 1.5413 | Training Accuracy 0.9786\n",
            "Epoch 00142 | Time(s) 0.2156 | Loss 1.5373 | Training Accuracy 0.9786\n",
            "Epoch 00143 | Time(s) 0.2151 | Loss 1.5334 | Training Accuracy 0.9786\n",
            "Epoch 00144 | Time(s) 0.2145 | Loss 1.5294 | Training Accuracy 0.9786\n",
            "Epoch 00145 | Time(s) 0.2139 | Loss 1.5253 | Training Accuracy 0.9786\n",
            "Epoch 00146 | Time(s) 0.2133 | Loss 1.5213 | Training Accuracy 0.9786\n",
            "Epoch 00147 | Time(s) 0.2129 | Loss 1.5173 | Training Accuracy 0.9786\n",
            "Epoch 00148 | Time(s) 0.2124 | Loss 1.5132 | Training Accuracy 0.9786\n",
            "Epoch 00149 | Time(s) 0.2127 | Loss 1.5092 | Training Accuracy 0.9786\n",
            "Epoch 00150 | Time(s) 0.2139 | Loss 1.5051 | Training Accuracy 0.9786\n",
            "\n",
            "Eval on validation dataset...\n",
            "Validation Accuracy: 0.7580\n",
            "\n",
            "Epoch 00151 | Time(s) 0.2144 | Loss 1.5010 | Training Accuracy 0.9786\n",
            "Epoch 00152 | Time(s) 0.2148 | Loss 1.4969 | Training Accuracy 0.9786\n",
            "Epoch 00153 | Time(s) 0.2160 | Loss 1.4928 | Training Accuracy 0.9786\n",
            "Epoch 00154 | Time(s) 0.2165 | Loss 1.4886 | Training Accuracy 0.9786\n",
            "Epoch 00155 | Time(s) 0.2173 | Loss 1.4845 | Training Accuracy 0.9786\n",
            "Epoch 00156 | Time(s) 0.2175 | Loss 1.4803 | Training Accuracy 0.9786\n",
            "Epoch 00157 | Time(s) 0.2179 | Loss 1.4762 | Training Accuracy 0.9786\n",
            "Epoch 00158 | Time(s) 0.2185 | Loss 1.4720 | Training Accuracy 0.9786\n",
            "Epoch 00159 | Time(s) 0.2196 | Loss 1.4678 | Training Accuracy 0.9786\n",
            "Epoch 00160 | Time(s) 0.2193 | Loss 1.4636 | Training Accuracy 0.9786\n",
            "Epoch 00161 | Time(s) 0.2189 | Loss 1.4594 | Training Accuracy 0.9786\n",
            "Epoch 00162 | Time(s) 0.2184 | Loss 1.4552 | Training Accuracy 0.9786\n",
            "Epoch 00163 | Time(s) 0.2180 | Loss 1.4509 | Training Accuracy 0.9786\n",
            "Epoch 00164 | Time(s) 0.2175 | Loss 1.4467 | Training Accuracy 0.9786\n",
            "Epoch 00165 | Time(s) 0.2172 | Loss 1.4424 | Training Accuracy 0.9786\n",
            "Epoch 00166 | Time(s) 0.2168 | Loss 1.4382 | Training Accuracy 0.9786\n",
            "Epoch 00167 | Time(s) 0.2166 | Loss 1.4339 | Training Accuracy 0.9786\n",
            "Epoch 00168 | Time(s) 0.2162 | Loss 1.4296 | Training Accuracy 0.9786\n",
            "Epoch 00169 | Time(s) 0.2158 | Loss 1.4253 | Training Accuracy 0.9786\n",
            "Epoch 00170 | Time(s) 0.2153 | Loss 1.4210 | Training Accuracy 0.9786\n",
            "Epoch 00171 | Time(s) 0.2149 | Loss 1.4167 | Training Accuracy 0.9786\n",
            "Epoch 00172 | Time(s) 0.2144 | Loss 1.4123 | Training Accuracy 0.9786\n",
            "Epoch 00173 | Time(s) 0.2140 | Loss 1.4080 | Training Accuracy 0.9786\n",
            "Epoch 00174 | Time(s) 0.2136 | Loss 1.4037 | Training Accuracy 0.9786\n",
            "Epoch 00175 | Time(s) 0.2133 | Loss 1.3993 | Training Accuracy 0.9786\n",
            "Epoch 00176 | Time(s) 0.2128 | Loss 1.3949 | Training Accuracy 0.9786\n",
            "Epoch 00177 | Time(s) 0.2124 | Loss 1.3906 | Training Accuracy 0.9786\n",
            "Epoch 00178 | Time(s) 0.2119 | Loss 1.3862 | Training Accuracy 0.9786\n",
            "Epoch 00179 | Time(s) 0.2115 | Loss 1.3818 | Training Accuracy 0.9786\n",
            "Epoch 00180 | Time(s) 0.2111 | Loss 1.3774 | Training Accuracy 0.9857\n",
            "\n",
            "Eval on validation dataset...\n",
            "Validation Accuracy: 0.7580\n",
            "\n",
            "Epoch 00181 | Time(s) 0.2107 | Loss 1.3730 | Training Accuracy 0.9857\n",
            "Epoch 00182 | Time(s) 0.2103 | Loss 1.3686 | Training Accuracy 0.9857\n",
            "Epoch 00183 | Time(s) 0.2098 | Loss 1.3641 | Training Accuracy 0.9857\n",
            "Epoch 00184 | Time(s) 0.2094 | Loss 1.3597 | Training Accuracy 0.9857\n",
            "Epoch 00185 | Time(s) 0.2090 | Loss 1.3553 | Training Accuracy 0.9857\n",
            "Epoch 00186 | Time(s) 0.2086 | Loss 1.3508 | Training Accuracy 0.9857\n",
            "Epoch 00187 | Time(s) 0.2082 | Loss 1.3464 | Training Accuracy 0.9857\n",
            "Epoch 00188 | Time(s) 0.2077 | Loss 1.3419 | Training Accuracy 0.9857\n",
            "Epoch 00189 | Time(s) 0.2074 | Loss 1.3374 | Training Accuracy 0.9857\n",
            "Epoch 00190 | Time(s) 0.2070 | Loss 1.3329 | Training Accuracy 0.9857\n",
            "Epoch 00191 | Time(s) 0.2066 | Loss 1.3284 | Training Accuracy 0.9857\n",
            "Epoch 00192 | Time(s) 0.2062 | Loss 1.3240 | Training Accuracy 0.9857\n",
            "Epoch 00193 | Time(s) 0.2058 | Loss 1.3195 | Training Accuracy 0.9857\n",
            "Epoch 00194 | Time(s) 0.2054 | Loss 1.3149 | Training Accuracy 0.9857\n",
            "Epoch 00195 | Time(s) 0.2051 | Loss 1.3104 | Training Accuracy 0.9857\n",
            "Epoch 00196 | Time(s) 0.2047 | Loss 1.3059 | Training Accuracy 0.9857\n",
            "Epoch 00197 | Time(s) 0.2044 | Loss 1.3014 | Training Accuracy 0.9857\n",
            "Epoch 00198 | Time(s) 0.2040 | Loss 1.2969 | Training Accuracy 0.9857\n",
            "Epoch 00199 | Time(s) 0.2036 | Loss 1.2923 | Training Accuracy 0.9857\n",
            "Epoch 00200 | Time(s) 0.2032 | Loss 1.2878 | Training Accuracy 0.9857\n",
            "Epoch 00201 | Time(s) 0.2029 | Loss 1.2832 | Training Accuracy 0.9857\n",
            "Epoch 00202 | Time(s) 0.2025 | Loss 1.2787 | Training Accuracy 0.9857\n",
            "Epoch 00203 | Time(s) 0.2022 | Loss 1.2741 | Training Accuracy 0.9857\n",
            "Epoch 00204 | Time(s) 0.2019 | Loss 1.2695 | Training Accuracy 0.9857\n",
            "Epoch 00205 | Time(s) 0.2016 | Loss 1.2650 | Training Accuracy 0.9857\n",
            "Epoch 00206 | Time(s) 0.2012 | Loss 1.2604 | Training Accuracy 0.9857\n",
            "Epoch 00207 | Time(s) 0.2010 | Loss 1.2558 | Training Accuracy 0.9857\n",
            "Epoch 00208 | Time(s) 0.2006 | Loss 1.2512 | Training Accuracy 0.9857\n",
            "Epoch 00209 | Time(s) 0.2003 | Loss 1.2467 | Training Accuracy 0.9857\n",
            "Epoch 00210 | Time(s) 0.2000 | Loss 1.2421 | Training Accuracy 0.9857\n",
            "\n",
            "Eval on validation dataset...\n",
            "Validation Accuracy: 0.7580\n",
            "\n",
            "Epoch 00211 | Time(s) 0.1997 | Loss 1.2375 | Training Accuracy 0.9857\n",
            "Epoch 00212 | Time(s) 0.1994 | Loss 1.2329 | Training Accuracy 0.9857\n",
            "Epoch 00213 | Time(s) 0.1991 | Loss 1.2283 | Training Accuracy 0.9857\n",
            "Epoch 00214 | Time(s) 0.1988 | Loss 1.2237 | Training Accuracy 0.9857\n",
            "Epoch 00215 | Time(s) 0.1986 | Loss 1.2191 | Training Accuracy 0.9857\n",
            "Epoch 00216 | Time(s) 0.1983 | Loss 1.2145 | Training Accuracy 0.9857\n",
            "Epoch 00217 | Time(s) 0.1980 | Loss 1.2099 | Training Accuracy 0.9857\n",
            "Epoch 00218 | Time(s) 0.1977 | Loss 1.2053 | Training Accuracy 0.9857\n",
            "Epoch 00219 | Time(s) 0.1975 | Loss 1.2006 | Training Accuracy 0.9857\n",
            "Epoch 00220 | Time(s) 0.1973 | Loss 1.1960 | Training Accuracy 0.9857\n",
            "Epoch 00221 | Time(s) 0.1970 | Loss 1.1914 | Training Accuracy 0.9857\n",
            "Epoch 00222 | Time(s) 0.1967 | Loss 1.1868 | Training Accuracy 0.9857\n",
            "Epoch 00223 | Time(s) 0.1964 | Loss 1.1822 | Training Accuracy 0.9857\n",
            "Epoch 00224 | Time(s) 0.1962 | Loss 1.1776 | Training Accuracy 0.9857\n",
            "Epoch 00225 | Time(s) 0.1959 | Loss 1.1729 | Training Accuracy 0.9857\n",
            "Epoch 00226 | Time(s) 0.1956 | Loss 1.1683 | Training Accuracy 0.9857\n",
            "Epoch 00227 | Time(s) 0.1954 | Loss 1.1637 | Training Accuracy 0.9857\n",
            "Epoch 00228 | Time(s) 0.1951 | Loss 1.1591 | Training Accuracy 0.9857\n",
            "Epoch 00229 | Time(s) 0.1948 | Loss 1.1544 | Training Accuracy 0.9857\n",
            "Epoch 00230 | Time(s) 0.1946 | Loss 1.1498 | Training Accuracy 0.9857\n",
            "Epoch 00231 | Time(s) 0.1943 | Loss 1.1452 | Training Accuracy 0.9857\n",
            "Epoch 00232 | Time(s) 0.1941 | Loss 1.1406 | Training Accuracy 0.9857\n",
            "Epoch 00233 | Time(s) 0.1938 | Loss 1.1359 | Training Accuracy 0.9857\n",
            "Epoch 00234 | Time(s) 0.1936 | Loss 1.1313 | Training Accuracy 0.9857\n",
            "Epoch 00235 | Time(s) 0.1934 | Loss 1.1267 | Training Accuracy 0.9857\n",
            "Epoch 00236 | Time(s) 0.1932 | Loss 1.1221 | Training Accuracy 0.9857\n",
            "Epoch 00237 | Time(s) 0.1929 | Loss 1.1175 | Training Accuracy 0.9857\n",
            "Epoch 00238 | Time(s) 0.1927 | Loss 1.1128 | Training Accuracy 0.9857\n",
            "Epoch 00239 | Time(s) 0.1925 | Loss 1.1082 | Training Accuracy 0.9857\n",
            "Epoch 00240 | Time(s) 0.1923 | Loss 1.1036 | Training Accuracy 0.9857\n",
            "\n",
            "Eval on validation dataset...\n",
            "Validation Accuracy: 0.7600\n",
            "\n",
            "Epoch 00241 | Time(s) 0.1922 | Loss 1.0990 | Training Accuracy 0.9857\n",
            "Epoch 00242 | Time(s) 0.1919 | Loss 1.0944 | Training Accuracy 0.9857\n",
            "Epoch 00243 | Time(s) 0.1918 | Loss 1.0898 | Training Accuracy 0.9857\n",
            "Epoch 00244 | Time(s) 0.1916 | Loss 1.0852 | Training Accuracy 0.9857\n",
            "Epoch 00245 | Time(s) 0.1914 | Loss 1.0806 | Training Accuracy 0.9857\n",
            "Epoch 00246 | Time(s) 0.1911 | Loss 1.0760 | Training Accuracy 0.9929\n",
            "Epoch 00247 | Time(s) 0.1909 | Loss 1.0714 | Training Accuracy 0.9929\n",
            "Epoch 00248 | Time(s) 0.1907 | Loss 1.0668 | Training Accuracy 0.9929\n",
            "Epoch 00249 | Time(s) 0.1906 | Loss 1.0622 | Training Accuracy 0.9929\n",
            "Epoch 00250 | Time(s) 0.1903 | Loss 1.0576 | Training Accuracy 0.9929\n",
            "Epoch 00251 | Time(s) 0.1901 | Loss 1.0530 | Training Accuracy 0.9929\n",
            "Epoch 00252 | Time(s) 0.1899 | Loss 1.0485 | Training Accuracy 0.9929\n",
            "Epoch 00253 | Time(s) 0.1897 | Loss 1.0439 | Training Accuracy 0.9929\n",
            "Epoch 00254 | Time(s) 0.1895 | Loss 1.0393 | Training Accuracy 0.9929\n",
            "Epoch 00255 | Time(s) 0.1894 | Loss 1.0348 | Training Accuracy 0.9929\n",
            "Epoch 00256 | Time(s) 0.1892 | Loss 1.0302 | Training Accuracy 0.9929\n",
            "Epoch 00257 | Time(s) 0.1890 | Loss 1.0257 | Training Accuracy 0.9929\n",
            "Epoch 00258 | Time(s) 0.1888 | Loss 1.0211 | Training Accuracy 0.9929\n",
            "Epoch 00259 | Time(s) 0.1886 | Loss 1.0166 | Training Accuracy 0.9929\n",
            "Epoch 00260 | Time(s) 0.1884 | Loss 1.0121 | Training Accuracy 0.9929\n",
            "Epoch 00261 | Time(s) 0.1882 | Loss 1.0076 | Training Accuracy 0.9929\n",
            "Epoch 00262 | Time(s) 0.1881 | Loss 1.0030 | Training Accuracy 0.9929\n",
            "Epoch 00263 | Time(s) 0.1879 | Loss 0.9985 | Training Accuracy 0.9929\n",
            "Epoch 00264 | Time(s) 0.1877 | Loss 0.9940 | Training Accuracy 0.9929\n",
            "Epoch 00265 | Time(s) 0.1876 | Loss 0.9896 | Training Accuracy 0.9929\n",
            "Epoch 00266 | Time(s) 0.1874 | Loss 0.9851 | Training Accuracy 0.9929\n",
            "Epoch 00267 | Time(s) 0.1872 | Loss 0.9806 | Training Accuracy 0.9929\n",
            "Epoch 00268 | Time(s) 0.1871 | Loss 0.9761 | Training Accuracy 0.9929\n",
            "Epoch 00269 | Time(s) 0.1871 | Loss 0.9717 | Training Accuracy 0.9929\n",
            "Epoch 00270 | Time(s) 0.1869 | Loss 0.9672 | Training Accuracy 0.9929\n",
            "\n",
            "Eval on validation dataset...\n",
            "Validation Accuracy: 0.7600\n",
            "\n",
            "Epoch 00271 | Time(s) 0.1867 | Loss 0.9628 | Training Accuracy 0.9929\n",
            "Epoch 00272 | Time(s) 0.1866 | Loss 0.9584 | Training Accuracy 0.9929\n",
            "Epoch 00273 | Time(s) 0.1865 | Loss 0.9539 | Training Accuracy 0.9929\n",
            "Epoch 00274 | Time(s) 0.1864 | Loss 0.9495 | Training Accuracy 0.9929\n",
            "Epoch 00275 | Time(s) 0.1863 | Loss 0.9451 | Training Accuracy 0.9929\n",
            "Epoch 00276 | Time(s) 0.1862 | Loss 0.9407 | Training Accuracy 0.9929\n",
            "Epoch 00277 | Time(s) 0.1861 | Loss 0.9363 | Training Accuracy 0.9929\n",
            "Epoch 00278 | Time(s) 0.1860 | Loss 0.9320 | Training Accuracy 0.9929\n",
            "Epoch 00279 | Time(s) 0.1858 | Loss 0.9276 | Training Accuracy 0.9929\n",
            "Epoch 00280 | Time(s) 0.1856 | Loss 0.9232 | Training Accuracy 0.9929\n",
            "Epoch 00281 | Time(s) 0.1854 | Loss 0.9189 | Training Accuracy 0.9929\n",
            "Epoch 00282 | Time(s) 0.1853 | Loss 0.9145 | Training Accuracy 0.9929\n",
            "Epoch 00283 | Time(s) 0.1851 | Loss 0.9102 | Training Accuracy 0.9929\n",
            "Epoch 00284 | Time(s) 0.1850 | Loss 0.9059 | Training Accuracy 0.9929\n",
            "Epoch 00285 | Time(s) 0.1848 | Loss 0.9016 | Training Accuracy 0.9929\n",
            "Epoch 00286 | Time(s) 0.1846 | Loss 0.8973 | Training Accuracy 0.9929\n",
            "Epoch 00287 | Time(s) 0.1845 | Loss 0.8930 | Training Accuracy 0.9929\n",
            "Epoch 00288 | Time(s) 0.1843 | Loss 0.8887 | Training Accuracy 0.9929\n",
            "Epoch 00289 | Time(s) 0.1841 | Loss 0.8845 | Training Accuracy 0.9929\n",
            "Epoch 00290 | Time(s) 0.1840 | Loss 0.8802 | Training Accuracy 0.9929\n",
            "Epoch 00291 | Time(s) 0.1839 | Loss 0.8760 | Training Accuracy 0.9929\n",
            "Epoch 00292 | Time(s) 0.1837 | Loss 0.8717 | Training Accuracy 0.9929\n",
            "Epoch 00293 | Time(s) 0.1835 | Loss 0.8675 | Training Accuracy 0.9929\n",
            "Epoch 00294 | Time(s) 0.1834 | Loss 0.8633 | Training Accuracy 0.9929\n",
            "Epoch 00295 | Time(s) 0.1832 | Loss 0.8591 | Training Accuracy 0.9929\n",
            "Epoch 00296 | Time(s) 0.1830 | Loss 0.8549 | Training Accuracy 0.9929\n",
            "Epoch 00297 | Time(s) 0.1829 | Loss 0.8507 | Training Accuracy 0.9929\n",
            "Epoch 00298 | Time(s) 0.1828 | Loss 0.8466 | Training Accuracy 0.9929\n",
            "Epoch 00299 | Time(s) 0.1826 | Loss 0.8424 | Training Accuracy 0.9929\n",
            "\n",
            "Test Accuracy 0.7640\n"
          ]
        }
      ],
      "source": [
        "# create the model, 2 heads, each head has hidden size 8\n",
        "model = GAT(g,\n",
        "          in_dim=features.size()[1],\n",
        "          hidden_dim=8,\n",
        "          out_dim=7,\n",
        "          num_heads=2)\n",
        "\n",
        "# create optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# main loop\n",
        "dur = []\n",
        "for epoch in range(300):\n",
        "    t0 = time.time()\n",
        "\n",
        "    logits = model(features)\n",
        "    logp = F.log_softmax(logits, 1)\n",
        "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
        "\n",
        "    train_acc = accuracy(logp[train_mask], labels[train_mask])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    dur.append(time.time() - t0)\n",
        "\n",
        "    print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Training Accuracy {:.4f}\".format(\n",
        "        epoch, np.mean(dur), loss.item(), train_acc))\n",
        "\n",
        "    if epoch % 30==0:\n",
        "        print(\"\\nEval on validation dataset...\")\n",
        "        val_acc = evaluate(model, features, labels, val_mask)\n",
        "        print(\"Validation Accuracy: {:.4f}\\n\".format(val_acc))\n",
        "\n",
        "print()\n",
        "acc = evaluate(model, features, labels, test_mask)\n",
        "print(\"Test Accuracy {:.4f}\".format(acc))\n"
      ]
    }
  ]
}